{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from inspect_ai import Task, eval\n",
    "from inspect_ai.dataset import MemoryDataset, Sample\n",
    "from inspect_ai.model import (\n",
    "    GenerateConfig,\n",
    "    get_model,\n",
    ")\n",
    "from inspect_ai.scorer import (\n",
    "    Score,\n",
    "    Scorer,\n",
    "    Target,\n",
    "    mean,\n",
    "    scorer,\n",
    "    std,\n",
    ")\n",
    "from inspect_ai.solver import TaskState\n",
    "from inspect_ai.util import StoreModel\n",
    "from loguru import logger\n",
    "from pydantic import Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Store(StoreModel):\n",
    "    weights: list[float] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(metrics=[mean(), std()])\n",
    "def weight_scorer() -> Scorer:\n",
    "    async def score(state: TaskState, target: Target) -> Score:\n",
    "        store = state.store_as(Store)\n",
    "        model_output = state.output.choices[0].message.text.strip()\n",
    "        logger.info(f\"Store: {store}\")\n",
    "        logger.info(f\"Model output: {model_output}\")\n",
    "\n",
    "        # Case 1: The model returns a number\n",
    "        try:\n",
    "            lbs = float(model_output)\n",
    "            return Score(value=lbs)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Case 2: The model yaps and then outputs a number. Essentially we need to find the last number in the output.\n",
    "        ## E.g. \"I think you can bench press 225 pounds\" -> 225\n",
    "        numbers = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", model_output)\n",
    "        if numbers:\n",
    "            lbs = float(numbers[-1])  # Take the last number found\n",
    "            return Score(value=lbs)\n",
    "\n",
    "        return Score(value=0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ## OpenAI\n",
    "    get_model(\"openai/gpt-4o-mini\", config=GenerateConfig(temperature=0.7)),\n",
    "    get_model(\"openai/gpt-4o\"),\n",
    "    get_model(\"openai/o3-mini\", config=GenerateConfig(reasoning_effort=\"high\")),\n",
    "    # get_model(\"gpt-4.5-preview\"),\n",
    "    ## Anthropic\n",
    "    get_model(\"anthropic/claude-3-5-sonnet-20240620\"),  # 3.5\n",
    "    get_model(\"anthropic/claude-3-5-sonnet-20241022\"),  # 3.6\n",
    "    get_model(\"anthropic/claude-3-7-sonnet-20250219\"),  # 3.7\n",
    "    get_model(\"anthropic/claude-3-opus-20240229\"),\n",
    "    ## Google\n",
    "    get_model(\"google/gemini-1.5-flash\", api_key=GOOGLE_API_KEY),\n",
    "    get_model(\"google/gemini-1.5-pro\", api_key=GOOGLE_API_KEY),\n",
    "    get_model(\"google/gemini-2.0-flash\", api_key=GOOGLE_API_KEY),\n",
    "    ## xAI\n",
    "    # get_model(\"xai/grok-3\"),  # api unavailable atm\n",
    "    # get_model(\"xai/grok-3-thinking\"),  # api unavailable atm\n",
    "    ## Meta\n",
    "    # get_model(\"?/Llama-3.1-405B-Instruct\"),  # Llama 3.1 405B\n",
    "    ## DeepSeek\n",
    "    # get_model(\"deepseek/deepseek-r1\"),\n",
    "]\n",
    "\n",
    "PROMPT = \"\"\"This is a fun question for \"BenchPressBench\", my new benchmark asking how much LLMs can benchpress.\n",
    "\n",
    "If you were a human, how much could you bench press?\n",
    "\n",
    "You may think as much as you want, but ultimately I want you to reply with a single number, in pounds, that answers the above question.\"\"\"\n",
    "\n",
    "results = eval(\n",
    "    Task(\n",
    "        dataset=MemoryDataset([Sample(input=PROMPT)]),\n",
    "        scorer=weight_scorer(),\n",
    "    ),\n",
    "    model=models,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
